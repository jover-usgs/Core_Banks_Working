{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate FloSup data into smaller sections using Arcpy\n",
    "## Write to Csv, return number of photos per section\n",
    "### Jin-Si R. Over - jover@usgs.gov\n",
    "### v1 -  7/23/2020\n",
    "### v2 - 8/24/2020\n",
    "### v3 - 8/27/2020 - added splitting sections and new zip names\n",
    "\n",
    "To Do: \n",
    "1. automate shapefiles and maps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ArcDesktop - 10.7 running python 2.7\n",
    "#Note, should run in python 3...\n",
    "\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    import archook #The module which locates arcgis\n",
    "    archook.get_arcpy()\n",
    "    import arcpy\n",
    "except ImportError:\n",
    "    print (\"can't find arcpy\")\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from scipy import interpolate, signal\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "import arcpy.cartography as CA\n",
    "\n",
    "arcpy.CheckOutExtension(\"Spatial\")\n",
    "arcpy.CheckOutExtension(\"3D\")\n",
    "env.overwriteOutput = True\n",
    "arcpy.overwriteOutput = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting workspace to the following directory: C:/Users/jover/Working/02_USGS/08_Aerial_Metadata/\n"
     ]
    }
   ],
   "source": [
    "#EDIT THIS CELL ONLY\n",
    "\n",
    "#Variables embedded in cells below\n",
    "release = 'postFlorence'  #Each flight is either post or pre 'Hurricane name' and then month abrv if more than more post flight. \n",
    "yearMonth = '2018-10'     #Change to reflect the flight date, if spanning over multiple months, choose the first date. \n",
    "state = '_NC_'            #Change for flight area name\n",
    "step = 0.05               #Change to set range of latitude \"bins\" (0.05 decimal degrees is 3 minutes)\n",
    "llb = 33.8                #lower latitudinal boundary, 33.8 is cape fear, 33.7 is the lowest part of NC\n",
    "ulb = 36.2                #upper latutudinal boundary, 36.2 encompases up to around Duck, 36.55 is VA/NC border\n",
    "l = ['a','b','c','d','e'] #Labels for split csvs. \n",
    "\n",
    "#Working Folder on Desktop\n",
    "wkspc = r\"C:/Users/jover/Working/02_USGS/08_Aerial_Metadata/\"                  \n",
    "\n",
    "#Metadata Folder - based on imgery to be released\n",
    "export_path = r\"C:/Users/jover/Working/02_USGS/08_Aerial_Metadata/\" + release\n",
    "\n",
    "print(\"Setting workspace to the following directory: \" + wkspc)\n",
    "\n",
    "\n",
    "# Set paths and names\n",
    "locflight           = wkspc + '01_RawData'  #Csv files for each flight\n",
    "locgdb \t\t\t\t= wkspc + release + '/' + release + '_Photo_Locations.gdb/'\t\t# Geodatabase location\n",
    "locshp\t\t\t\t= wkspc + '02_Shapefiles/'\t\t\t# Shapefiles named as release + \"PhotoLoc_all\"- should be from a merged csv for each flight if on multiple days)\n",
    "loccsv              = wkspc + release + '/FlightZips/' #Place the names for each section\n",
    "env.workspace       = locgdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.85\n",
      "33\n",
      "151.0\n",
      "2018-10_NC_3351N.csv\n"
     ]
    }
   ],
   "source": [
    "#Set up floating ranges to call in a loop\n",
    "first = np.arange(llb, ulb, step)                  #Lower bounding range of latitude floats that I want to separate out\n",
    "second = np.arange(llb + step, ulb + step, step)   #Upper bounding range based on step\n",
    "#zips = range(0,len(first),1)                       #Just an index for naming the output csvs\n",
    "\n",
    "#Test naming conventions for the zip files - this may not work if step changes?\n",
    "f = round(first[1],2)             #Round the step to make sure increments by 3\n",
    "print(f)\n",
    "print(int(str(f)[0:2]))           #Call the first two numbers (degrees)\n",
    "print(float(str(f)[2:5])*60+100)  #Call the second two numbers as a decimal and convert to minutes (add 100 to avoid zero problems)\n",
    "print(yearMonth + state + str(f)[0:2] + str(float(str(f)[2:5])*60+100)[1:3] + 'N' + '.csv') #Put it all together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to select out file names from Florence by .05 (or whatever step specifed) decimal degrees\n",
    "\n",
    "for i in range(len(first)):\n",
    "    #Make a working layer to access from a point shapefile containing all the locations/data\n",
    "    #The shapefile should be all the Flight Locations csv files merged with only one heading\n",
    "    arcpy.MakeFeatureLayer_management(locshp + release + \"_PhotoLoc_all.shp\", release + '_PhotoLoc_lyr')\n",
    "    \n",
    "    #Select from the layer by the latitude attribute and the ranges specified in the cell above\n",
    "    sectionPhotos = arcpy.SelectLayerByAttribute_management(release + '_PhotoLoc_lyr',\"NEW_SELECTION\",\"Latitude > \" + str(first[i]) + \" AND Latitude < \" + str(second[i]))\n",
    "    \n",
    "    #Creates copy of the selected features and puts them into a gdb feature class that is overwritten each time\n",
    "    arcpy.CopyFeatures_management(sectionPhotos, locgdb + \"overwrite\")\n",
    "    \n",
    "    #Takes the file being overwritten and makes it into a csv with an incremental name based off of the date, state, and latitude 4 digit code\n",
    "    # TableToTable takes three inputs, feature class, output location, output name.\n",
    "    f = round(first[i],2)\n",
    "    zname = yearMonth + state + str(f)[0:2] + str(float(str(f)[2:5])*60+100)[1:3] + 'N'  #Output name - change parts in cell above\n",
    "    \n",
    "    arcpy.TableToTable_conversion(locgdb + \"overwrite\", loccsv, zname + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of extraneous columns in the csv created by Arc\n",
    "\n",
    "#Walk through the created csv, read them using pandas and then rewrite it only using the columns we want to keep\n",
    "for f in glob.glob(loccsv + \"*.csv\"):\n",
    "    fn = pd.read_csv(f)\n",
    "    keep_col = ['Station','UTC_Time', 'Latitude', 'Longitude', 'Easting','Northing']\n",
    "    new_f = fn[keep_col]\n",
    "    new_f.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No split necessary for <open file '2018-10_NC_3348N.csv', mode 'r' at 0x000000001D442C00>\n",
      "No split necessary for <open file '2018-10_NC_3351N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3354N.csv', mode 'r' at 0x000000001D442C00>\n",
      "No split necessary for <open file '2018-10_NC_3357N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3400N.csv', mode 'r' at 0x000000001D442C00>\n",
      "No split necessary for <open file '2018-10_NC_3403N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3406N.csv', mode 'r' at 0x000000001D442C00>\n",
      "No split necessary for <open file '2018-10_NC_3409N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3412N.csv', mode 'r' at 0x000000001D442C00>\n",
      "No split necessary for <open file '2018-10_NC_3415N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3418N.csv', mode 'r' at 0x000000001D442C00>\n",
      "No split necessary for <open file '2018-10_NC_3421N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3424N.csv', mode 'r' at 0x000000001D442C00>\n",
      "No split necessary for <open file '2018-10_NC_3427N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3430N.csv', mode 'r' at 0x000000001D442C00>\n",
      "There are 2321 photos, splitting into four sections with 580 in each...\n",
      "There are 3007 photos, splitting into five sections with 601 in each...\n",
      "There are 3278 photos, splitting into five sections with 655 in each...\n",
      "No split necessary for <open file '2018-10_NC_3515N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3518N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3521N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3524N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3527N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3530N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3533N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3536N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3539N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3542N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3545N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3548N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3551N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3557N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3600N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3603N.csv', mode 'r' at 0x000000001D442E40>\n",
      "No split necessary for <open file '2018-10_NC_3606N.csv', mode 'r' at 0x000000001D442C90>\n",
      "No split necessary for <open file '2018-10_NC_3612N.csv', mode 'r' at 0x000000001D442E40>\n"
     ]
    }
   ],
   "source": [
    "#Split CSVs in multiple -a, -b, etc sections if length is greater than 750 images. Doesn't delete the pre-split csv - fix?\n",
    "os.chdir(loccsv)\n",
    "\n",
    "for fn in glob.glob('*N.csv'):      #Walk through every csv other than the Photo_count if its in the same folder \n",
    "    with open(fn) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        rows = [row for row in reader]\n",
    "        row_count = len(rows)             #Count number of rows in the csv\n",
    "        \n",
    "        #Do some basic math to determine how many csvs will need to be made, at the moment the code only goes up to five splits (a-e)\n",
    "        d2 = int(round(row_count/2,0))\n",
    "        d3 = int(round(row_count/3,0))\n",
    "        d4 = int(round(row_count/4,0))\n",
    "        d5 = int(round(row_count/5,0))\n",
    "        #print(row_count)\n",
    "        \n",
    "        if row_count < 750:\n",
    "            print('No split necessary for %s' % f)\n",
    "        \n",
    "        elif  d2 < 750:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into two sections with %d in each...' % (row_count,d2))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d2+2)])   #add 2 to prevent issues with an odd split adding an extra csv with only one line\n",
    "                start_index += d2+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:  #Name the split csvs - if naming convention changes need to change the [0:16]\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)        #Keep headers when splitting\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            #If split, close the file and delete it before moving on\n",
    "            f.close()\n",
    "            os.remove(fn)\n",
    "        elif d3 < 750:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into three sections with %d in each...' % (row_count,d3))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d3+2)])\n",
    "                start_index += d3+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            f.close()\n",
    "            os.remove(fn)         \n",
    "        elif d4 < 750:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into four sections with %d in each...' % (row_count,d4))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d4+2)])\n",
    "                start_index += d4+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            f.close()\n",
    "            os.remove(fn)              \n",
    "        elif d5 < 750:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into five sections with %d in each...' % (row_count,d5))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d5+2)])\n",
    "                start_index += d5+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            f.close()\n",
    "            os.remove(fn)               \n",
    "        else:\n",
    "            print('Manually split because this is getting ridiculous')\n",
    "                            \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write out how many photos are in each file and write out as a csv - https://stackoverflow.com/questions/31616217/python-script-to-count-num-lines-in-all-files-in-directory\n",
    "#The csv needs to be transposed though...\n",
    "\n",
    "#os.chdir('C:/Users/jover/Working/02_USGS/01b_OBX_Florence/01_Flight_Zip_Sections/')\n",
    "#os.chdir('C:/Users/jover/Working/02_USGS/01b_OBX_Florence/001_FlightZips/')\n",
    "os.chdir(loccsv)\n",
    "names={}\n",
    "for fn in glob.glob('*N*.csv'):                          #Walk through every csv other than the  (fixed so it won't count the Photo_count if its in the same folder) in the zipped sections\n",
    "    with open(fn) as f:\n",
    "        names[fn]=sum(1 for line in f if line.strip()) #counts the lines and sums them for each csv\n",
    "\n",
    "#print(names)        \n",
    "with open('Photo_count.csv','wb') as f:                #Save as a csv\n",
    "    w = csv.DictWriter(f,names.keys());\n",
    "    w.writeheader();\n",
    "    w.writerow(names)                                  #First row is the names of the csvs, second is the count\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
