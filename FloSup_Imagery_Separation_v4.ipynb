{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate FloSup data into smaller sections using CSV or shapefile and Arcpy\n",
    "## Write to Csv, return number of photos per section\n",
    "### Jin-Si R. Over - jover@usgs.gov\n",
    "### v1 -  7/23/2020\n",
    "### v2 - 8/24/2020\n",
    "### v3 - 8/27/2020 - added splitting sections and new zip names\n",
    "### v4 - 9/3/2020 - added ability to work without using arc, only use cell 4 OR 5, not both\n",
    "\n",
    "To Do: \n",
    "1. automate shapefiles and maps?\n",
    "2. If split, write out gdb files of the split csvs for mapping?\n",
    "3. Tranpose PhotoCount.csv when its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ArcDesktop - 10.7 running python 2.7\n",
    "#Note, should run in python 3...\n",
    "\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    import archook #The module which locates arcgis\n",
    "    archook.get_arcpy()\n",
    "    import arcpy\n",
    "except ImportError:\n",
    "    print (\"can't find arcpy\")\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from scipy import interpolate, signal\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "import arcpy.cartography as CA\n",
    "\n",
    "arcpy.CheckOutExtension(\"Spatial\")\n",
    "arcpy.CheckOutExtension(\"3D\")\n",
    "env.overwriteOutput = True\n",
    "arcpy.overwriteOutput = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting workspace to the following directory: C:/Users/jover/Working/02_USGS/08_Aerial_Metadata/\n"
     ]
    }
   ],
   "source": [
    "#EDIT THIS CELL ONLY\n",
    "\n",
    "#Variables embedded in cells below\n",
    "release = 'postFlorence'  #Each flight is either post or pre 'Hurricane name' and then month abrv if more than more post flight. \n",
    "yearMonth = '2018-10'     #Change to reflect the flight date, if spanning over multiple months, choose the first date. \n",
    "state = '_NC_'            #Change for flight area name\n",
    "step = 0.05               #Change to set range of latitude \"bins\" (0.05 decimal degrees is 3 minutes)\n",
    "llb = 33.8                #lower latitudinal boundary, 33.8 is cape fear, 33.7 is the lowest part of NC\n",
    "ulb = 36.2                #upper latutudinal boundary, 36.2 encompases up to around Duck, 36.55 is VA/NC border\n",
    "l = ['a','b','c','d','e'] #Labels for split csvs. \n",
    "limit = 750               #Max number of photos in a zip file, 750 is roughly 20 GB if each photo is ~20 MB\n",
    "\n",
    "#Can change to decide which columns to keep in final csvs\n",
    "keepCol = ['Station','UTCTime', 'Latitude', 'Longitude', 'Easting','Northing'] \n",
    "\n",
    "#Working Folder on Desktop\n",
    "wkspc = r\"C:/Users/jover/Working/02_USGS/08_Aerial_Metadata/\"                  \n",
    "\n",
    "#Metadata Folder - based on imgery to be released\n",
    "export_path = r\"C:/Users/jover/Working/02_USGS/08_Aerial_Metadata/\" + release\n",
    "\n",
    "print(\"Setting workspace to the following directory: \" + wkspc)\n",
    "\n",
    "\n",
    "# Set paths and names\n",
    "locRaw              = wkspc + '01_RawData/'  #Csv files for each flight\n",
    "locgdb \t\t\t\t= wkspc + release + '/' + release + '_Photo_Locations.gdb/'\t\t# Geodatabase location\n",
    "locshp\t\t\t\t= wkspc + '02_Shapefiles/'\t\t\t# Shapefiles named as release + \"PhotoLoc_all\"- should be from a merged csv for each flight if on multiple days)\n",
    "loccsv              = wkspc + release + '/FlightZips/' #Place the names for each section\n",
    "env.workspace       = locgdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.85\n",
      "33\n",
      "151.0\n",
      "2018-10_NC_3351N.csv\n"
     ]
    }
   ],
   "source": [
    "#Set up floating ranges to call in a loop\n",
    "first = np.arange(llb, ulb, step)                  #Lower bounding range of latitude floats that I want to separate out\n",
    "second = np.arange(llb + step, ulb + step, step)   #Upper bounding range based on step\n",
    "#zips = range(0,len(first),1)                       #Just an index for naming the output csvs\n",
    "\n",
    "#Test naming conventions for the zip files - this may not work if step changes?\n",
    "f = round(first[1],2)             #Round the step to make sure increments by 3\n",
    "print(f)\n",
    "print(int(str(f)[0:2]))           #Call the first two numbers (degrees)\n",
    "print(float(str(f)[2:5])*60+100)  #Call the second two numbers as a decimal and convert to minutes (add 100 to avoid zero problems)\n",
    "print(yearMonth + state + str(f)[0:2] + str(float(str(f)[2:5])*60+100)[1:3] + 'N' + '.csv') #Put it all together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF USING THIS CELL DO NOT RUN THE NEXT ONE - IT DOES THE SAME THING IF A SHAPEFILE IS AVAILABLE\n",
    "\n",
    "#Loop to select out file names from Flights by step using the csv in the 01_RawData folder\n",
    "\n",
    "#Which csv to read\n",
    "df = pd.read_csv(locRaw + release + '_Photo_Locations_all.csv')\n",
    "\n",
    "#Loop for latitude intervals\n",
    "for j in range(len(first)):\n",
    "    temp = df[(df['Latitude'] >= first[j]) & (df['Latitude'] < second[j])]\n",
    "    \n",
    "    #Naming conventions\n",
    "    f = round(first[j],2)\n",
    "    zname = yearMonth + state + str(f)[0:2] + str(float(str(f)[2:5])*60+100)[1:3] + 'N'  #Output name - change parts in cell above\n",
    "    \n",
    "    #Create the new csv\n",
    "    temp.to_csv(loccsv + zname + '.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS IF ALREADY USING PREVIOUS CELL - WILL OVERWRITE FOR NO REASON, note this cell will also create auxilary files\n",
    "\n",
    "\n",
    "#Loop to select out file names from Flights by .05 (or whatever step specifed) decimal degrees using a shapefile\n",
    "\n",
    "for i in range(len(first)):\n",
    "    #Make a working layer to access from a point shapefile containing all the locations/data\n",
    "    #The shapefile should be all the Flight Locations csv files merged with only one heading\n",
    "    arcpy.MakeFeatureLayer_management(locshp + release + \"_PhotoLoc_all.shp\", release + '_PhotoLoc_lyr')\n",
    "    \n",
    "    #Select from the layer by the latitude attribute and the ranges specified in the cell above\n",
    "    sectionPhotos = arcpy.SelectLayerByAttribute_management(release + '_PhotoLoc_lyr',\"NEW_SELECTION\",\"Latitude >= \" + str(first[i]) + \" AND Latitude < \" + str(second[i]))\n",
    "    \n",
    "    #Creates copy of the selected features and puts them into a gdb feature class that is overwritten each time\n",
    "    arcpy.CopyFeatures_management(sectionPhotos, locgdb + \"overwrite\")\n",
    "    \n",
    "    #Takes the file being overwritten and makes it into a csv with an incremental name based off of the date, state, and latitude 4 digit code\n",
    "    # TableToTable takes three inputs, feature class, output location, output name.\n",
    "    f = round(first[i],2)\n",
    "    zname = yearMonth + state + str(f)[0:2] + str(float(str(f)[2:5])*60+100)[1:3] + 'N'  #Output name - change parts in cell above\n",
    "    \n",
    "    arcpy.TableToTable_conversion(locgdb + \"overwrite\", loccsv, zname + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of extraneous columns in the newly csvs\n",
    "\n",
    "#Walk through the created csv, read them using pandas and then rewrite it only using the columns we want to keep\n",
    "for f in glob.glob(loccsv + \"*.csv\"):\n",
    "    fn = pd.read_csv(f)\n",
    "    new_f = fn[keepCol]\n",
    "    new_f.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f92b0fdf25be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mrow_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;31m#Count number of rows in the csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtotalPhotos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'There are a total of %d photos'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtotalPhotos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "#Split CSVs in multiple -a, -b, etc sections if length is greater than 750 (or set the limit variable) images. Deletes the pre-split csv. \n",
    "#Note that the splits are based purely on math, they aren't by date or smaller sections of the latitude. \n",
    "os.chdir(loccsv)\n",
    "\n",
    "for fn in glob.glob('*N.csv'):      #Walk through every csv other than the Photo_count if its in the same folder \n",
    "    with open(fn) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        rows = [row for row in reader]\n",
    "        row_count = len(rows)             #Count number of rows in the csv\n",
    "        \n",
    "        #Do some basic math to determine how many csvs will need to be made, at the moment the code only goes up to five splits (a-e)\n",
    "        d2 = int(round(row_count/2,0))\n",
    "        d3 = int(round(row_count/3,0))\n",
    "        d4 = int(round(row_count/4,0))\n",
    "        d5 = int(round(row_count/5,0))\n",
    "        #print(row_count)\n",
    "        \n",
    "        if row_count < limit:\n",
    "            print('No split necessary for %s' % f)\n",
    "        \n",
    "        elif  d2 < 750:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into two sections with %d in each...' % (row_count,d2))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d2+2)])   #add 2 to prevent issues with an odd split adding an extra csv with only one line\n",
    "                start_index += d2+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:  #Name the split csvs - if naming convention changes need to change the [0:16]\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)        #Keep headers when splitting\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            #If split, close the file and delete it before moving on\n",
    "            f.close()\n",
    "            os.remove(fn)\n",
    "        elif d3 < limit:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into three sections with %d in each...' % (row_count,d3))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d3+2)])\n",
    "                start_index += d3+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            f.close()\n",
    "            os.remove(fn)         \n",
    "        elif d4 < limit:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into four sections with %d in each...' % (row_count,d4))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d4+2)])\n",
    "                start_index += d4+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            f.close()\n",
    "            os.remove(fn)              \n",
    "        elif d5 < limit:\n",
    "            pages = []\n",
    "            print('There are %d photos, splitting into five sections with %d in each...' % (row_count,d5))\n",
    "            start_index = 0\n",
    "            while start_index < row_count:\n",
    "                pages.append(rows[start_index: start_index + (d5+2)])\n",
    "                start_index += d5+2\n",
    "            for i, page in enumerate(pages):\n",
    "                with open('{}-{}.csv'.format(fn[0:16], l[i]), 'wb') as outfile:\n",
    "                    writer = csv.DictWriter(outfile, fieldnames = header)\n",
    "                    writer.writeheader()\n",
    "                    for row in page:\n",
    "                          writer.writerow(row)\n",
    "            f.close()\n",
    "            os.remove(fn)               \n",
    "        else:\n",
    "            print('Manually split because this is getting ridiculous')\n",
    "                            \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write out how many photos are in each file and write out as a csv - https://stackoverflow.com/questions/31616217/python-script-to-count-num-lines-in-all-files-in-directory\n",
    "#The csv needs to be transposed though...\n",
    "\n",
    "#os.chdir('C:/Users/jover/Working/02_USGS/01b_OBX_Florence/01_Flight_Zip_Sections/')\n",
    "#os.chdir('C:/Users/jover/Working/02_USGS/01b_OBX_Florence/001_FlightZips/')\n",
    "os.chdir(loccsv)\n",
    "names={}\n",
    "for fn in glob.glob('*N*.csv'):                          #Walk through every csv other than the  (fixed so it won't count the Photo_count if its in the same folder) in the zipped sections\n",
    "    with open(fn) as f:\n",
    "        names[fn]=sum(1 for line in f if line.strip())-1 #counts the lines, minus the header and sums them for each csv\n",
    "        \n",
    "with open('Photo_count.csv','wb') as f:                #Save as a csv\n",
    "    w = csv.DictWriter(f,names.keys());\n",
    "    w.writeheader();\n",
    "    w.writerow(names)                                  #First row is the names of the csvs, second is the count\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
